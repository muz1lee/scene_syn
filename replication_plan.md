

import torch
import torch.optim as optim

def run_scenethesis_system():
    # -------------------------------------------------
    # 0. åŸºç¡€è®¾æ–½åˆå§‹åŒ–
    # -------------------------------------------------
    db_assets = ["bed", "sofa", "desk", "chair", "table", "laptop", "plant", "lamp", "bookshelf"]
    env_maps = ["sunny.hdr", "cloudy.hdr"]
    
    # å®ä¾‹åŒ–æ¨¡å—
    planner = CoarseScenePlanner(db_assets)
    refiner = VisualRefinementModule(db_assets, env_maps)
    physics_engine = PhysicsOptimizer(device='cuda')
    judge = SceneJudge(threshold=0.7)
    
    # åˆå§‹åŒ–å¯å¾®æ¸²æŸ“å™¨ (Phase 3 éœ€è¦ç”¨åˆ°)
    # renderer = DifferentiableRenderer(device='cuda') 

    user_prompt = "A messy bedroom"
    max_retries = 3
    current_try = 0

    # -------------------------------------------------
    # Re-planning Loop (Phase 4 é—­ç¯æ§åˆ¶)
    # -------------------------------------------------
    while current_try < max_retries:
        print(f"\nğŸš€ === å°è¯•ç¬¬ {current_try + 1} æ¬¡ç”Ÿæˆ ===")

        # --- Phase 1: Planning ---
        # ä½ çš„ä»£ç : ç”Ÿæˆç‰©ä½“æ¸…å•å’Œç²—ç•¥æè¿°
        plan = planner.run_pipeline(user_prompt)
        
        # --- Phase 2: Visual Refinement ---
        # ä½ çš„ä»£ç : ç”Ÿæˆå‚è€ƒå›¾ï¼Œåˆå§‹ 3D å¸ƒå±€ (æ­¤æ—¶ç‰©ä½“å¯èƒ½ç©¿æ¨¡/æ‚¬ç©º)
        initial_layout = refiner.process_layout(plan)
     
        # -------------------------------------------------
        # Phase 3: Physics Optimization (æ ¸å¿ƒç¼ºå¤±éƒ¨åˆ†)
        # -------------------------------------------------
        print("\nğŸ”¨ [Phase 3] å¼€å§‹ç‰©ç†ä¸å§¿æ€ä¼˜åŒ–...")
        
        # 1. å°† Layout è½¬æ¢ä¸ºå¯ä¼˜åŒ–çš„ PyTorch å‚æ•° (T, R, s)
        # è¿™ä¸€æ­¥éœ€è¦æŠŠ initial_layout é‡Œçš„å­—å…¸è½¬æˆ requires_grad=True çš„ Tensor
        scene_graph_params = prepare_optimization_params(initial_layout) 
        
        # 2. è®¾ç½®ä¼˜åŒ–å™¨ (è®ºæ–‡å¼ºè°ƒä½¿ç”¨ SGD)
        optimizer = optim.SGD(scene_graph_params.parameters(), lr=0.01, momentum=0.9)
        
        # 3. ä¼˜åŒ–å¾ªç¯
        for i in range(200): # è¿­ä»£ 200 æ¬¡
            optimizer.zero_grad()
            
            # è®¡ç®—æ€» Loss (Pose + Collision + Stability)
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¼ å…¥ renderer æ¥è®¡ç®— Pose Loss
            loss = physics_engine(scene_graph_params, initial_layout['image_guidance'])
            
            loss.backward()
            optimizer.step()
            
            if i % 50 == 0:
                print(f"    Iter {i}: Loss = {loss.item():.4f}")

        # è·å–ä¼˜åŒ–åçš„æœ€ç»ˆå¸ƒå±€
        final_layout = export_layout(scene_graph_params)

        # -------------------------------------------------
        # Phase 4: Judge (è£åˆ¤)
        # -------------------------------------------------
        # æ¸²æŸ“æœ€ç»ˆç»“æœç»™è£åˆ¤çœ‹ (è¿™é‡Œéœ€è¦æ¸²æŸ“å™¨ç”Ÿæˆä¸€å¼ å›¾)
        # final_render_img = renderer.render(final_layout) 
        # è¿™é‡Œæš‚æ—¶ç”¨å ä½ç¬¦ä»£æ›¿
        final_render_img = "final_render_placeholder.jpg" 

        passed, report = judge.evaluate(final_render_img, initial_layout['image_guidance'])
        
        if passed:
            print("\n æˆåŠŸï¼ç”Ÿæˆç¬¦åˆç‰©ç†è§„å¾‹ä¸”è§†è§‰å¯¹é½çš„åœºæ™¯ã€‚")
            print("FINAL JSON:", json.dumps(final_layout, indent=2))
            break
        else:
            print(f"\nâŒ å¤±è´¥: {report['reasoning']}")
            print("ğŸ”„ è§¦å‘ Re-planning...")
            current_try += 1
            # å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹ user_prompt æˆ–å¢åŠ éšæœºç§å­æ¥è·å¾—ä¸åŒç»“æœ

    if current_try >= max_retries:
        print("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç”Ÿæˆå¤±è´¥ã€‚")

# è¾…åŠ©å‡½æ•°: æ¨¡æ‹Ÿå‚æ•°è½¬æ¢
class SceneGraphModel(torch.nn.Module):
    def __init__(self, layout_dict):
        super().__init__()
        # å°† T, R, s å˜æˆå¯å­¦ä¹ å‚æ•°
        self.objects = torch.nn.ParameterList([
            torch.nn.Parameter(torch.randn(3)) for _ in layout_dict['scene_layout']
        ])
    def forward(self):
        return self.objects

def prepare_optimization_params(layout):
    return SceneGraphModel(layout)

def export_layout(model):
    return {"optimized": True, "data": "..."} 

if __name__ == "__main__":
    run_scenethesis_system()

1. Coarse Scene Planning
import json
from typing import List, Dict, Any

# ==========================================
# 1. è®ºæ–‡ Section 7.1: æ ¸å¿ƒæç¤ºè¯æ¨¡æ¿
# ==========================================
SCENE_PLANNING_SYSTEM_PROMPT = Coarse_Scene_Planning_Instruction_Prompts
class CoarseScenePlanner:
    def __init__(self, database_assets: List[str]):
        self.db_assets = database_assets
        self.db_set = set(asset.lower() for asset in database_assets) # ä¼˜åŒ–æŸ¥è¯¢é€Ÿåº¦

    def run_pipeline(self, user_input: str) -> Dict[str, Any]:
        """
        ä¸»å…¥å£å‡½æ•°
        """
        # 1. åˆ†æ”¯å¤„ç† (Branching)
        if self._is_simple_prompt(user_input):
            print(f"--- æ£€æµ‹åˆ°ç®€å• Prompt: '{user_input}' -> è¿›å…¥ç”Ÿæˆæ¨¡å¼ ---")
            return self._process_simple_mode(user_input)
        else:
            print(f"--- æ£€æµ‹åˆ°è¯¦ç»† Prompt -> è¿›å…¥éªŒè¯æ¨¡å¼ ---")
            return self._process_detailed_mode(user_input)

    # ==========================================
    # è·¯å¾„ A: Flexible Scene Generation (ç®€å•æ¨¡å¼)
    # ==========================================
    def _process_simple_mode(self, simple_prompt: str) -> Dict[str, Any]:
        """
        åˆ©ç”¨ Section 7.1 çš„ Promptï¼Œè®© LLM è‡ªå·±æ¨ç†ã€é€‰å“ã€å®šé”šç‚¹
        """
        # æ„é€ è¾“å…¥æ¶ˆæ¯
        user_message = f"""
        [Database Assets]: {", ".join(self.db_assets)}
        [User Prompt]: "{simple_prompt}"
        """

        # æ¨¡æ‹Ÿè°ƒç”¨ LLM (GPT-4o)
        # è¿™é‡Œçš„å…³é”®æ˜¯ System Prompt åŒ…å«äº† Section 7.1 çš„æ‰€æœ‰çº¦æŸ
        response_json = self._mock_llm_call(
            system_prompt=SCENE_PLANNING_SYSTEM_PROMPT, 
            user_message=user_message
        )
        
        # ç®€å•æ¨¡å¼ä¸‹ï¼ŒLLM çš„è¾“å‡ºç›´æ¥å°±æ˜¯æœ€ç»ˆç»“æœï¼Œå› ä¸º Prompt é‡Œå·²ç»è¦æ±‚å®ƒå®šå¥½ anchor å’Œ spatial relations äº†
        return {
            "mode": "simple_generated",
            "anchor": response_json["anchor_object"],
            "objects": response_json["selected_objects"],
            "detailed_description": response_json["upsampled_prompt"]
        }

    # ==========================================
    # è·¯å¾„ B: Controllable Scene Generation (ä¸“å®¶/è¯¦ç»†æ¨¡å¼)
    # ==========================================
    def _process_detailed_mode(self, detailed_prompt: str) -> Dict[str, Any]:
        """
        åŸæ–‡é€»è¾‘ï¼šChecks for presence -> Infers categories -> Skips up-sampling -> Identifies anchor
        """
        # 1. æå–å®ä½“ (NER)
        raw_objects = self._extract_entities(detailed_prompt)
        
        # 2. æŸ¥åº“éªŒè¯ (Review & Check Presence)
        valid_objects = []
        for obj in raw_objects:
            # å°è¯•ç›´æ¥åŒ¹é…
            if obj.lower() in self.db_set:
                valid_objects.append(obj.lower())
            else:
                # å°è¯•æ¨æ–­ (Infer relevant categories)
                # ä¾‹å¦‚ï¼šç”¨æˆ·å†™ "Macbook", åº“é‡Œæœ‰ "Laptop"
                inferred = self._infer_category(obj)
                if inferred:
                    valid_objects.append(inferred)
                else:
                    print(f"Warning: å¿½ç•¥æœªçŸ¥ç‰©ä½“ '{obj}'")
        
        if not valid_objects:
            raise ValueError("æ— æ³•åœ¨è¯¦ç»†æè¿°ä¸­åŒ¹é…åˆ°ä»»ä½•æ•°æ®åº“èµ„äº§")

        # 3. ç¡®å®šé”šç‚¹ (Identifies an anchor object)
        # è¯¦ç»†æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬æœ‰äº†ç‰©ä½“åˆ—è¡¨ï¼Œä½†éœ€è¦æ‰¾å‡ºè°æ˜¯è€å¤§
        # å†æ¬¡è°ƒç”¨ä¸€ä¸ªå°å‹çš„ LLM ä»»åŠ¡ï¼Œéµå¾ª Holodeck ç­–ç•¥
        anchor = self._identify_anchor_logic(valid_objects)

        # 4. è·³è¿‡ä¸Šé‡‡æ · (Skip up-sampling)ï¼Œç›´æ¥ä½¿ç”¨ç”¨æˆ·è¾“å…¥ä½œä¸ºæè¿°
        # ä½†æˆ‘ä»¬éœ€è¦å»ºç«‹å±‚çº§å…³ç³» (Coarse spatial hierarchy)
        return {
            "mode": "detailed_controlled",
            "anchor": anchor,
            "objects": valid_objects,
            "detailed_description": detailed_prompt # åŸå°ä¸åŠ¨ä¿ç•™ç”¨æˆ·çš„è¯¦ç»†æè¿°
        }

    # ==========================================
    # è¾…åŠ©ä¸æ¨¡æ‹Ÿæ–¹æ³• (Helpers)
    # ==========================================
    def _is_simple_prompt(self, text: str) -> bool:
        # ç®€å•åˆ¤å®šï¼šé•¿åº¦çŸ­ï¼Œæˆ–è€…ç¼ºå°‘ä»‹è¯æ–¹ä½è¯(on, next to, behind)
        return len(text.split()) < 10

    def _extract_entities(self, text: str) -> List[str]:
        # å®é™…åº”è¯¥è°ƒç”¨ NLP æ¨¡å‹ï¼Œè¿™é‡Œæ¨¡æ‹Ÿæå–
        # å‡è®¾è¾“å…¥: "A chair next to a table" -> ["chair", "table"]
        # è¿™é‡Œä»…ä½œæ¼”ç¤º
        import re
        words = re.findall(r'\w+', text.lower())
        return [w for w in words if w in self.db_set or w in ["macbook", "sofa"]] 

    def _infer_category(self, obj: str) -> str:
        # æ¨¡æ‹Ÿè¯­ä¹‰æ˜ å°„
        mapping = {"macbook": "laptop", "seat": "chair", "flowerpot": "plant"}
        val = mapping.get(obj.lower())
        return val if val in self.db_set else None

    def _identify_anchor_logic(self, object_list: List[str]) -> str:
        """
        å¯¹åº”åŸæ–‡: "occupying the highest spatial hierarchy apart from the ground"
        """
        # å¯ä»¥ç”¨ LLMï¼Œä¹Ÿå¯ä»¥ç”¨ç¡¬è§„åˆ™ã€‚è¿™é‡Œæ¨¡æ‹Ÿ LLM å†³ç­–ã€‚
        priority = ["bed", "sofa", "table", "desk", "bookshelf", "cabinet"]
        for p in priority:
            if p in object_list:
                return p
        return object_list[0] # Fallback

    def _mock_llm_call(self, system_prompt, user_message):
        """
        æ¨¡æ‹Ÿ GPT-4o çš„ JSON è¿”å›
        """
        # å‡è®¾ Simple Prompt æ˜¯ "A messy bedroom"
        return {
            "anchor_object": "bed",
            "selected_objects": ["bed", "desk", "chair", "laptop", "books", "clothes", "lamp"],
            "upsampled_prompt": "The bed is the central anchor against the back wall. A desk is placed next to the bed..."
        }

"""
Task Description:
You are responsible for generating a set of common objects and planning a scene based on these common objects. You will be given a list that includes all available object categories and a text prompt to describe a scene. This is a hard task, please think deeply and write down your analysis in following steps:

Step 1: Review All Categories
    a. Begin by thoroughly reviewing the categories in the provided list.
    b. Identify potential groups or clusters of objects within this list that are commonly found in similar environments (e.g., furniture, electronics, household items, etc.).
    
Step 2: Interpret Input Prompt
    a. Carefully read the input prompt. Understand the theme, primary activities, or the setting it describes, as these will guide your object selection. i.e. if the prompt gives: children playing room, then you may think of objects like tent, toy, bear, ball, chair, etc.

Step 3: Object Selection
    a. Based on the description, select at least 15 object categories from the list that match the scene.
    b. Determine the anchor object: i. Identify the anchor object among the selected objects. Consider the following factors:
        1. A large object directly on the ground (i.e. floor, table, or shelf).
        2. An object that influences where other objects are placed (i.e. a table in a dining room, and there are cups and fruits on the table).
        3. The object should logically anchor the scene and often defines the sceneâ€™s layout orientation. i.e. the sofa in a front-facing view in the scene.
    
Step 4: Object Cross-check
    a. I will give you $100 tips if you can cross-check whether objects in the scene can be found in the given category list or its relevant categories. i.e., if there is a bookshelf in your planned scene, the bookshelf should also be found in the given list, or bookcase can be found in the list if bookshelf is not covered by the category. Otherwise, re-plan the scene.

Step 5: Plan Scene with Selected Objects
    a. Based on the description and selected objects, plan the scene, keeping these aspects in mind:
        i. Functionality: Choose objects that are contextually relevant to the scene (e.g., selecting a table, chair, flower vase, and utensils for a dining room), but do not generate any wall d Ìecor objects.
        ii. Spatial Hierarchy:
            1. Please have a depth effect in the layout. For the depth effect, the scene should have some objects placed on the ground as the background, central, and in the front, resulting in a depth layout. i.e. the sofa and bookshelf are the background of the table and chair set in the living room.
            2. Please have a supportive item in the layout. i.e. the shoes, bag, and hat are in the display shelf in a clothes store, where the display shelf is a supportive item.
        iii. Balance: Ensure a mix of large and small objects to avoid overcrowding or under-populating the scene. i.e. taking the table as the center, there are flower vases, fruits, and cups on the table, and chairs are on the sides.

Step 6: Output Format:
    a. Save the selected objects as a json file follow the output format:
        Anchor object:
        Other common objects:
    b. Save scene planning as txt file.
"""
2. Layout Visual Refinement (è§†è§‰å¸ƒå±€ç»†åŒ–)

Step 1: Image Guidance (ä»¥å›¾å¼•è·¯)
- è¾“å…¥ï¼š ç¬¬ä¸€éƒ¨åˆ†ç”Ÿæˆçš„ detailed_description 
- åŠ¨ä½œï¼š è°ƒç”¨æ–‡ç”Ÿå›¾æ¨¡å‹  GPT-4o (DALL-E 3) ã€‚
- ç›®çš„ï¼š åˆ©ç”¨ç”Ÿæˆæ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®é›†å­¦åˆ°çš„â€œç‰©ä½“å…±ç°â€å’Œâ€œç©ºé—´å…³ç³»â€ï¼Œç”Ÿæˆä¸€å¼ çœ‹èµ·æ¥å¾ˆåˆç†çš„ 2D å‚è€ƒå›¾ã€‚è¿™å¼ å›¾å°±æ˜¯åç»­ 3D å¸ƒå±€çš„è“å›¾ã€‚

Step 2: Scene Graph Generation (åŒè½¨åœºæ™¯å›¾æ„å»º)
- é€»è¾‘è½¨ (GPT-4o): å®šä¹‰åœºæ™¯æ ‘ç»“æ„ï¼ˆè°æ˜¯åœ°åŸº Groundï¼Œè°æ˜¯çˆ¶èŠ‚ç‚¹ Parentï¼Œè°æ˜¯å­èŠ‚ç‚¹ Childï¼‰ã€‚æ³¨æ„ï¼š è¿™ä¸€æ­¥ä¸æ¶‰åŠåƒç´ ï¼Œåªæ¶‰åŠç‰©ä½“é—´çš„å±‚çº§é€»è¾‘ã€‚
- å‡ ä½•è½¨ (Grounded-SAM + Depth Pro): å¤„ç†åƒç´ ã€‚
  - Grounded-SAM: åˆ†å‰² Mask + è£å‰ªå›¾ç‰‡ (Cropped Images)ã€‚
  - Depth Pro: ä¼°ç®— Metric Depthã€‚
  - Lifting: 2D åƒç´  -> 3D ç‚¹äº‘ -> 3D Bounding Boxã€‚
- åˆå¹¶: å°†å‡ ä½•åæ ‡å¡«å…¥é€»è¾‘æ ‘ä¸­ã€‚

Step 3: Asset Retrieval (èµ„äº§æ£€ç´¢)
- åŠ¨ä½œ A (ç‰©ä½“æ£€ç´¢): ä½¿ç”¨ CLIP (ViT-L/14)ã€‚
  - è¾“å…¥ï¼šGrounded-SAM æ‰£å‡ºæ¥çš„ç‰©ä½“åˆ‡ç‰‡å›¾ã€‚
  - æ•°æ®åº“ï¼šObjaverseã€‚
  - åŸç†ï¼šå¯¹æ¯”â€œåˆ‡ç‰‡å›¾â€å’Œâ€œ3Dèµ„äº§ç¼©ç•¥å›¾â€çš„è¯­ä¹‰ç‰¹å¾ï¼Œæ‰¾æœ€åƒçš„ã€‚
- åŠ¨ä½œ B (ç¯å¢ƒæ£€ç´¢ - æ–°å¢): ä½¿ç”¨ GPT-4oã€‚
  - ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªï¼Ÿ è®ºæ–‡æ˜ç¡®è¯´ Scenethesis åªç”Ÿæˆåœ°é¢ç‰©ä½“ (objects on the ground)ã€‚å¢™å£ã€çª—æˆ·ã€é˜³å…‰ã€æµ·æ»©èƒŒæ™¯ç­‰ï¼Œä¸ç”Ÿæˆ 3D æ¨¡å‹ï¼Œè€Œæ˜¯ç›´æ¥ä»æ•°æ®åº“é‡Œæ‰¾ä¸€å¼ åŒ¹é…çš„ HDRI ç¯å¢ƒè´´å›¾ã€‚
  - è¾“å…¥ï¼šUpsampled Prompt (æ–‡æœ¬)ã€‚
  - è¾“å‡ºï¼šEnvironment Map (e.g., "sunny_beach.hdr")ã€‚

import json
import random
from typing import List, Dict, Any

class VisualRefinementModule:
    def __init__(self, asset_database: List[str], env_map_database: List[str]):
        self.asset_db = asset_database
        self.env_db = env_map_database
        print("--- [ç³»ç»Ÿåˆå§‹åŒ–] Loading Models ---")
        print("  |-- Visual Gen: GPT-4o (DALL-E 3 Integration)")
        print("  |-- Segmentation: Grounded-SAM")
        print("  |-- Depth Estimation: Depth Pro")
        print("  |-- Retrieval: CLIP (ViT-L/14) + GPT-4o (EnvMap)")

    def process_layout(self, coarse_plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¯¹åº”è®ºæ–‡ 6.2.2 æµç¨‹
        """
        detailed_prompt = coarse_plan["detailed_description"]
        print(f"\n[Phase 2 Start] Visual Refinement for: '{detailed_prompt[:30]}...'")

        # Step 1: Image Guidance (GPT-4o/DALL-E 3)
        # "GPT-4o generates an image to serve as fine-grained layout guidance"
        generated_image = self._generate_image_gpt4o(detailed_prompt)

        # Step 2: Scene Graph Construction (Hybrid: Logic + Geometry)
        # "GPT-4o ... define ground/parent/child" + "Grounded-SAM ... segments"
        scene_graph = self._construct_scene_graph(
            image=generated_image,
            object_list=coarse_plan["objects"],
            anchor_name=coarse_plan["anchor"]
        )

        # Step 3: Asset Retrieval (CLIP)
        # "CLIP ... retrieve 3D assets that align with image guidance"
        final_objects = self._retrieve_3d_assets_clip(scene_graph)

        # Step 4: Environment Map Selection (GPT-4o)
        # "GPT-4o is further utilized to select the most relevant environment map"
        selected_env_map = self._select_env_map_gpt4o(detailed_prompt)

        return {
            "image_guidance": generated_image,
            "scene_layout": final_objects,
            "environment_map": selected_env_map
        }

    # =========================================================================
    # æ ¸å¿ƒå­å‡½æ•°å®ç°
    # =========================================================================

    def _generate_image_gpt4o(self, prompt: str) -> str:
        # æ¨¡æ‹Ÿè°ƒç”¨ DALL-E 3
        print(f"  [1. Image Gen] Generating guidance image via GPT-4o...")
        return "fake_image_tensor_640x640"

    def _construct_scene_graph(self, image, object_list, anchor_name):
        print(f"  [2. Scene Graph] Constructing 3D Spatial Graph...")
        
        # A. é€»è¾‘å±‚çº§ (GPT-4o)
        # è®ºæ–‡æåˆ° GPT-4o è´Ÿè´£å®šä¹‰ Ground -> Parent -> Child å…³ç³»
        # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†é˜²æ­¢å‡ ä½•è®¡ç®—å‡ºé”™ï¼ˆæ¯”å¦‚æŠŠæ•å¤´ç®—åˆ°åœ°æ¿ä¸Šï¼‰
        hierarchy = self._gpt4o_define_hierarchy(object_list, anchor_name)
        
        # B. å‡ ä½•æ„ŸçŸ¥ (Grounded-SAM + Depth Pro)
        # è®ºæ–‡: "Segments each object... projected into 3D space using Depth Pro"
        nodes = []
        for obj_name in object_list:
            # 1. Segment (SAM) -> å¾—åˆ° Mask å’Œ Crop
            mask, cropped_img = self._sam_segment(image, obj_name)
            
            # 2. Depth Project (Depth Pro) -> å¾—åˆ° 3D åæ ‡
            # Initial positioning within a spatial relationship graph
            pos_3d, bbox_3d = self._depth_pro_lift(mask)
            
            nodes.append({
                "label": obj_name,
                "role": hierarchy.get(obj_name, "child"), # ä» GPT-4o è·å–è§’è‰²
                "parent": hierarchy.get(f"{obj_name}_parent", "ground"),
                "initial_pose": {"pos": pos_3d, "bbox": bbox_3d},
                "visual_crop": cropped_img # ç”¨äº CLIP æ£€ç´¢
            })
            
        return nodes

    def _retrieve_3d_assets_clip(self, scene_graph):
        print(f"  [3. Asset Retrieval] Matching assets using CLIP (ViT-L/14)...")
        for node in scene_graph:
            # æ¨¡æ‹Ÿ CLIP å‘é‡æœç´¢
            # Query: node['visual_crop'] (å›¾åƒç‰¹å¾) + node['label'] (è¯­ä¹‰ç‰¹å¾)
            # Key: Database Assets
            best_match_id = f"{node['label']}_premium_v1.glb"
            node["asset_id"] = best_match_id
            print(f"    - Matched '{node['label']}' -> {best_match_id}")
        return scene_graph

    def _select_env_map_gpt4o(self, prompt):
        print(f"  [4. Env Map] Selecting HDR Environment Map via GPT-4o...")
        # é€»è¾‘ï¼šæŠŠ prompt ç»™ GPT-4oï¼Œè®©å®ƒä» env_db é‡Œé€‰ä¸€ä¸ª
        # æ¯”å¦‚ prompt æ˜¯ "sunset beach"ï¼ŒGPT-4o ä¼šé€‰ "beach_sunset_4k.hdr"
        selected = random.choice(self.env_db)
        print(f"    - Selected Context: {selected}")
        return selected

    # --- æ¨¡æ‹Ÿåº•å±‚æ¨¡å‹ ---
    def _gpt4o_define_hierarchy(self, objects, anchor):
        # ç®€å•æ¨¡æ‹Ÿ GPT-4o è¿”å›çš„å±‚çº§å…³ç³»
        # å‡è®¾ Anchor æ˜¯ Parentï¼Œå…¶ä»–éƒ½æ˜¯ Child
        h = {anchor: "parent"}
        for o in objects:
            if o != anchor:
                h[o] = "child"
                h[f"{o}_parent"] = anchor # è®°å½•è°æ˜¯è°çš„çˆ¶èŠ‚ç‚¹
        return h

    def _sam_segment(self, image, label):
        return "mask_array", "cropped_image_tensor"

    def _depth_pro_lift(self, mask):
        # æ¨¡æ‹Ÿä» Depth Pro ç®—å‡ºçš„åæ ‡
        return [random.uniform(-2,2), 0, random.uniform(-2,2)], [1, 1, 1]



3. Physics-aware Optimization

åœ¨ä¸Šä¸€é˜¶æ®µï¼ˆVisual Refinementï¼‰ï¼Œç³»ç»Ÿè™½ç„¶ç»™å‡ºäº† 3D å¸ƒå±€ï¼Œä½†å®ƒåªæ˜¯åŸºäº 2D å›¾ç‰‡åæ¨çš„ï¼Œå­˜åœ¨ä¸¥é‡çš„**â€œå¹»è§‰â€å’Œè¯¯å·®**ï¼š
- é®æŒ¡è¯¯å·®ï¼š å›¾ç‰‡é‡Œæ¡Œå­è¢«æ¤…å­æŒ¡ä½äº†ï¼Œåæ¨å‡ºæ¥çš„æ¡Œå­å¯èƒ½åªæœ‰ä¸€åŠã€‚
- ç©¿æ¨¡ä¸æ‚¬ç©ºï¼š ç‰©ä½“å¯èƒ½ä¼šæ’è¿›å¢™é‡Œï¼Œæˆ–è€…æµ®åœ¨åŠç©ºä¸­ã€‚
- å½¢çŠ¶å·®å¼‚ï¼š æ•°æ®åº“é‡Œæ‰¾å‡ºæ¥çš„æ¨¡å‹å’Œå›¾ç‰‡é‡Œçš„æ¨¡å‹é•¿å¾—ä¸å®Œå…¨ä¸€æ ·ï¼ˆæ¯”å¦‚å›¾ç‰‡æ˜¯åœ†è§’æ¡Œï¼Œæ¨¡å‹æ˜¯ç›´è§’æ¡Œï¼‰ã€‚
Physics-aware Optimization çš„ä½œç”¨å°±æ˜¯ï¼šé€šè¿‡æ•°å­¦ä¼˜åŒ–ï¼ŒæŠŠè¿™äº›æ¨¡å‹â€œæ¨â€åˆ°æ­£ç¡®çš„ä½ç½®ï¼Œæ—¢è¦å¯¹é½å‚è€ƒå›¾ï¼Œåˆè¦ç¬¦åˆç‰©ç†è§„å¾‹ã€‚

---
æ ¸å¿ƒæ­¥éª¤è¯¦è§£
è¿™ä¸ªè¿‡ç¨‹ä¸æ˜¯ä¸€æ¬¡æ€§çš„è®¡ç®—ï¼Œè€Œæ˜¯ä¸€ä¸ªè¿­ä»£ä¼˜åŒ–å¾ªç¯ (Iterative Loop)ã€‚ç³»ç»Ÿå®šä¹‰äº†ä¸€ä¸ªæ€»èƒ½é‡å‡½æ•°ï¼ˆTotal Lossï¼‰ï¼Œé€šè¿‡ä¸æ–­å¾®è°ƒç‰©ä½“çš„ 5-DoF å‚æ•°ï¼ˆç¼©æ”¾ $s$ã€æ—‹è½¬ $R$ã€å¹³ç§» $T$ï¼‰ï¼Œè®©èƒ½é‡é™åˆ°æœ€ä½ã€‚
æ­¥éª¤ 1ï¼šå§¿æ€å¯¹é½ (Pose Alignment) â€”â€” â€œçœ‹ç€è¦åƒâ€
è¿™æ­¥æ˜¯ä¸ºäº†è§£å†³â€œæ¨¡å‹æ‘†æ”¾ä½ç½®ä¸å¯¹â€çš„é—®é¢˜ã€‚
- æŠ€æœ¯æ ¸å¿ƒï¼š Dense Semantic Correspondence (ç¨ å¯†è¯­ä¹‰åŒ¹é…)ï¼Œä½¿ç”¨ RoMa æ¨¡å‹ã€‚
- ä¸ºä»€ä¹ˆä¸ç”¨åƒç´ å¯¹æ¯”ï¼Ÿ å› ä¸ºæ£€ç´¢åˆ°çš„ 3D èµ„äº§çº¹ç†å’Œç”Ÿæˆå›¾ä¸ä¸€æ ·ï¼Œç›´æ¥æ¯”åƒç´ ï¼ˆMSE Lossï¼‰ä¼šå½»åº•å¤±è´¥ã€‚RoMa æ¯”è¾ƒçš„æ˜¯â€œè¯­ä¹‰ç‰¹å¾â€ï¼Œæ¯”å¦‚å®ƒçŸ¥é“â€œè¿™æ˜¯æ¡Œè…¿â€ï¼Œä¸ç®¡æ¡Œè…¿æ˜¯é»‘æ˜¯ç™½ã€‚
- å…·ä½“æ“ä½œï¼š
  1. æ¸²æŸ“ï¼š æŠŠå½“å‰çš„ 3D åœºæ™¯æ¸²æŸ“æˆ 2D å›¾åƒ $I$ã€‚
  2. åŒ¹é…ï¼š æ‹¿è¿™å¼ å›¾ $I$ å’Œ Phase 2 ç”Ÿæˆçš„å‚è€ƒå›¾ $\tilde{I}$ è¿›è¡Œç‰¹å¾ç‚¹åŒ¹é…ã€‚ç³»ç»Ÿä¼šç­›é€‰å‡ºç½®ä¿¡åº¦ $\tau \ge 0.6$ çš„ $m=100$ å¯¹å…³é”®ç‚¹ã€‚
  3. ä¼˜åŒ– ($L_{pose}$): è°ƒæ•´ 3D æ¨¡å‹çš„å‚æ•°ï¼Œè®©è¿™ 100 ä¸ªç‚¹åœ¨ 3D ç©ºé—´å’Œ 2D æŠ•å½±ç©ºé—´ä¸Šçš„è·ç¦»æœ€å°åŒ–ã€‚

æ­¥éª¤ 2ï¼šç‰©ç†åˆç†æ€§ (Physical Plausibility) â€”â€” â€œç‰©ç†è¦çœŸâ€
è¿™æ­¥æ˜¯ä¸ºäº†è§£å†³â€œç©¿æ¨¡â€å’Œâ€œæ‚¬ç©ºâ€é—®é¢˜ã€‚Scenethesis æŠ›å¼ƒäº†ç²—ç³™çš„ Bounding Boxï¼Œæ”¹ç”¨ SDF (ç¬¦å·è·ç¦»åœº) æ¥å®ç°é«˜ç²¾åº¦ç¢°æ’æ£€æµ‹ã€‚
ç³»ç»Ÿä¼šåœ¨ç‰©ä½“è¡¨é¢å‡åŒ€é‡‡æ · $n=400$ ä¸ªç‚¹æ¥æ¢æµ‹ç¯å¢ƒã€‚
A. é˜²ç¢°æ’ï¼šå¹³ç§»æ¨ç¦» ($L_{translation}$)
- ç°è±¡ï¼š å¦‚æœç‰©ä½“è¡¨é¢çš„ç‚¹æ£€æµ‹åˆ° SDF å€¼ $d < 0$ï¼Œè¯´æ˜æ’è¿›åˆ«äººä½“å†…äº†ã€‚
- åŠ¨ä½œï¼š è®¡ç®—ä¸€ä¸ªæ¨åŠ›å‘é‡ $u$ï¼ˆä»ç¢°æ’ç‚¹æŒ‡å‘ç‰©ä½“è´¨å¿ƒï¼‰ã€‚ç³»ç»Ÿä¼šæŠŠç‰©ä½“æ²¿ç€ $u$ æ–¹å‘æ¨å¼€ï¼Œæ¨å¼€çš„è·ç¦»å°±æ˜¯ç©¿æ¨¡æ·±åº¦ $|d|$ã€‚
- ç›´è§‚ç†è§£ï¼š å°±åƒä¸¤ä¸ªç£é“åŒæç›¸æ–¥ï¼Œç¦»å¾—è¶Šè¿‘ï¼ˆç©¿æ¨¡è¶Šæ·±ï¼‰ï¼Œæ¨å¼€çš„åŠ›è¶Šå¤§ã€‚
B. é˜²ç¢°æ’ï¼šæŒ¤å‹ç¼©å° ($L_{scale}$)
- ç°è±¡ï¼š å¦‚æœä¸€ä¸ªç‰©ä½“è¢«ä»ä¸¤ä¸ªä¸åŒçš„æ–¹å‘åŒæ—¶æŒ¤å‹ï¼ˆæ¯”å¦‚ä¹¦è¢«å¤¹åœ¨ä¸¤å±‚ä¹¦æ¶ä¹‹é—´ï¼Œæˆ–è€…è¢«å·¦å³ä¸¤æœ¬ä¹¦å¤¹ä½ï¼‰ï¼Œå…‰é å¹³ç§»æ˜¯æ¨ä¸å‡ºæ¥çš„ã€‚
- åˆ¤æ–­é€»è¾‘ï¼š ç³»ç»Ÿæ£€æµ‹ç¢°æ’ç‚¹çš„æ–¹å‘ï¼Œå¦‚æœèšç±»æ•° $N_{cluster} > 1$ï¼ˆå³æ¥è‡ªä¸åŒæ–¹å‘çš„ç¢°æ’ï¼‰ï¼Œå°±åˆ¤å®šä¸ºâ€œè¢«å¤¹å‡»â€ã€‚
- åŠ¨ä½œï¼š å‡å°ç‰©ä½“çš„ç¼©æ”¾å€¼ $s$ï¼Œè®©å®ƒå˜å°ä¸€ç‚¹ï¼Œç›´åˆ°èƒ½å¡è¿›å»ã€‚
C. ç¨³å®šæ€§ï¼šé‡åŠ›å¸é™„ ($L_{stability}$)
- ç°è±¡ï¼š ç‰©ä½“ä¸èƒ½æ‚¬æµ®ã€‚
- åŠ¨ä½œï¼š é‡‡æ ·ç‰©ä½“åº•éƒ¨çš„ç‚¹ $V^B$ã€‚ç³»ç»Ÿå¼ºåˆ¶è¦æ±‚è¿™äº›ç‚¹ç›¸å¯¹äºâ€œçˆ¶èŠ‚ç‚¹è¡¨é¢â€ï¼ˆæ¯”å¦‚æ¡Œé¢ï¼‰çš„ SDF å€¼å¿…é¡»ä¸º 0ã€‚
- æ•°å­¦å®ç°ï¼š ä½¿ç”¨ Loss $= \sum (1 - \exp(-d^2))$ã€‚å¦‚æœä¸è´´åˆï¼ŒLoss å°±å¤§ï¼›è´´åˆäº†ï¼ŒLoss å°±æ˜¯ 0ã€‚è¿™ç›¸å½“äºç»™ç‰©ä½“æ–½åŠ äº†ä¸€ä¸ªâ€œæ•°å­—é‡åŠ›â€ã€‚

---
å…³é”®å·¥ç¨‹ç»†èŠ‚ (å¤ç°å¿…è¯»)
æ ¹æ® Section 6.3 å’Œ Method Overviewï¼Œå¤ç°æ—¶å¿…é¡»éµå®ˆä»¥ä¸‹â€œå†›è§„â€ï¼š
1. ä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ (Two-stage Optimization):
  - Phase A: å…ˆåªè·‘ Pose Alignmentã€‚è®©ç‰©ä½“å…ˆé£åˆ°å¤§æ¦‚æ­£ç¡®çš„ä½ç½®ã€‚
  - Phase B: å†åŠ å…¥ Physical Constraintsã€‚åœ¨è§†è§‰å¯¹é½çš„åŸºç¡€ä¸Šï¼ŒæŠŠç‰©ä½“â€œæŒ‰â€åœ¨åœ°ä¸Šï¼Œå¹¶æ¨å¼€é‡å éƒ¨åˆ†ã€‚
  - åŸå› ï¼š å¦‚æœä¸€å¼€å§‹å°±å¼€ç‰©ç†ç¢°æ’ï¼Œç‰©ä½“å¯èƒ½ä¼šè¢«å¼¹é£ï¼Œå¯¼è‡´è§†è§‰åŒ¹é…å¤±è´¥ã€‚
2. ä¼˜åŒ–å™¨é€‰æ‹©ï¼šSGD (Stochastic Gradient Descent)
  - ä¸è¦ç”¨ Adamï¼ è®ºæ–‡æ˜ç¡®æŒ‡å‡º Adam çš„åŠ¨é‡æœºåˆ¶ (Adaptive Momentum) ä¼šå¯¼è‡´ä¼˜åŒ–ä¸ç¨³å®šï¼ˆå› ä¸ºç‰©ç†ç¢°æ’çš„æ¢¯åº¦æ˜¯çªå˜çš„ï¼‰ï¼ŒSGD æ•ˆæœæ›´ç¨³ã€‚
3. æŠ€æœ¯æ ˆï¼š
  - åº•å±‚åŸºäº PyTorch3D å®ç°ï¼ˆå› ä¸ºéœ€è¦å¯å¾®æ¸²æŸ“å’Œ 3D æ“ä½œï¼‰ã€‚
  - åœ¨ A100 GPU ä¸Šè¿è¡Œã€‚
æ€»ç»“æµç¨‹å›¾
1. è¾“å…¥ï¼š ç²—ç³™çš„ Scene Graph + 2D å‚è€ƒå›¾ã€‚
2. é¢„å¤„ç†ï¼š å°† 3D èµ„äº§è½¬æ¢ä¸º SDF åœºï¼Œé‡‡æ ·è¡¨é¢ç‚¹ ($n=400$)ã€‚
3. å¾ªç¯è¿­ä»£ (SGD):
  - è®¡ç®—è§†è§‰ Loss ($L_{pose}$): RoMa æ‰¾ç‚¹ -> ç®—è·ç¦» -> æ¢¯åº¦å›ä¼ ã€‚
  - è®¡ç®—ç‰©ç† Loss ($L_{phy}$): æŸ¥ SDF -> ç®—ç©¿æ¨¡æ·±åº¦ -> æ–½åŠ æ¨åŠ›/ç¼©åŠ›/é‡åŠ›ã€‚
  - æ›´æ–°å‚æ•°: ä¿®æ”¹ $T$ (ä½ç§»), $R$ (æ—‹è½¬), $s$ (å¤§å°)ã€‚
4. è£åˆ¤ (Judge): GPT-4o æœ€ç»ˆçœ‹ä¸€çœ¼ï¼ˆSection 3.4ï¼‰ï¼Œåˆæ ¼åˆ™è¾“å‡ºã€‚
5. è¾“å‡ºï¼š ä¸€ä¸ªè§†è§‰å¯¹é½ã€æ— ç©¿æ¨¡ã€æ¥è§¦ç´§å®çš„å®Œç¾ 3D åœºæ™¯ã€‚

import torch
import torch.nn as nn
import torch.nn.functional as F

class PhysicsOptimizer(nn.Module):
    def __init__(self, device='cuda'):
        super().__init__()
        self.device = device
        
        # è¶…å‚æ•°è®¾ç½®
        self.n_samples = 400         # n surface points
        self.m_correspondence = 100  # m matching points
        self.tau = 0.6               # confidence threshold
        
        # Loss æƒé‡ç³»æ•° (éœ€è¦æ ¹æ®å®éªŒè°ƒæ•´)
        self.lambda_p = 1.0   # Pose
        self.lambda_cT = 10.0 # Collision Translation
        self.lambda_cS = 10.0 # Collision Scale
        self.lambda_s = 5.0   # Stability

    def forward(self, scene_graph, ref_image_data):
        """
        å¯¹åº”è®ºæ–‡ Eq (9): L = Î»p*Lpose + Î»cT*Ltranslation + Î»cS*Lscale + Î»s*Lstability
        """
        # 1. å§¿æ€å¯¹é½æŸå¤± (Eq 4, 5)
        loss_pose = self._compute_pose_loss(scene_graph, ref_image_data)
        
        # 2. ç‰©ç†ç¢°æ’ä¸ç¨³å®šæ€§æŸå¤±
        loss_trans, loss_scale, loss_stab = 0.0, 0.0, 0.0
        
        for obj in scene_graph.objects:
            # é‡‡æ ·ç‰©ä½“è¡¨é¢ç‚¹ n=400
            surface_points = self._sample_surface(obj, self.n_samples)
            # è®¡ç®—è¿™äº›ç‚¹åœ¨åœºæ™¯ä¸­çš„ SDF å€¼ (d)
            # d < 0 è¡¨ç¤ºç¢°æ’ (Inside), d > 0 è¡¨ç¤ºå®‰å…¨ (Outside)
            sdf_values, gradients = self._query_scene_sdf(surface_points, scene_graph)
            
            # --- Eq 6: Translation Collision Loss ---
            loss_trans += self._compute_translation_loss(obj, sdf_values, gradients)
            
            # --- Eq 7: Scale Collision Loss ---
            loss_scale += self._compute_scale_loss(obj, sdf_values, gradients)
            
            # --- Eq 8: Stability Loss ---
            loss_stab += self._compute_stability_loss(obj, scene_graph)

        # æ€»æŸå¤±
        total_loss = (self.lambda_p * loss_pose + 
                      self.lambda_cT * loss_trans + 
                      self.lambda_cS * loss_scale + 
                      self.lambda_s * loss_stab)
        
        return total_loss

    # =========================================================
    # å…·ä½“çš„å…¬å¼å®ç° (Mapping Formulas to Code)
    # =========================================================

    def _compute_translation_loss(self, obj, sdf, gradients):
        """
        å¯¹åº” L_translation
        å…¬å¼: || f(T, |d|, u) - T ||^2
        """
        # 1. ç­›é€‰ç¢°æ’ç‚¹ V- (SDF < 0)
        mask_collision = sdf < 0
        if not mask_collision.any():
            return 0.0
            
        d_collided = sdf[mask_collision]    # è´Ÿçš„ SDF å€¼
        grad_collided = gradients[mask_collision] # ç¢°æ’ç‚¹çš„æ¢¯åº¦æ–¹å‘
        
        # 2. å®šä¹‰æ–¹å‘ u (ä»ç¢°æ’ç‚¹æŒ‡å‘ç‰©ä½“è´¨å¿ƒ)
        # ç®€åŒ–å®ç°ï¼šé€šå¸¸ SDF çš„æ¢¯åº¦æ–¹å‘å°±æ˜¯æ¨å¼€ç¢°æ’çš„æœ€å¿«æ–¹å‘
        u = -grad_collided 
        
        # 3. è®¡ç®—åç§»é‡ |d| * u
        # f(T,...) - T å…¶å®å°±æ˜¯è®¡ç®—éœ€è¦ç§»åŠ¨çš„å‘é‡ delta
        # Eq 6: |d| = max(0, -d(vi))ï¼Œå³ç»å¯¹å€¼
        push_vector = u * torch.abs(d_collided).unsqueeze(1)
        
        # 4. L2 Loss
        return torch.sum(push_vector ** 2)

    def _compute_scale_loss(self, obj, sdf, gradients):
        """
        å¯¹åº” L_scale
        é€»è¾‘: å¦‚æœç‰©ä½“è¢«ä¸¤è¾¹å¤¹ä½ (N_cluster > 1)ï¼Œå°±ç¼©å°å®ƒ
        """
        mask_collision = sdf < 0
        if mask_collision.sum() < 2:
            return 0.0

        # 1. èšç±»æ£€æµ‹ (N_cluster)
        # ç®€å•åˆ¤åˆ«ï¼šå¦‚æœç¢°æ’ç‚¹çš„æ¢¯åº¦æ–¹å‘å·®å¼‚å¾ˆå¤§(æ¯”å¦‚ç‚¹ç§¯ < -0.5)ï¼Œè¯´æ˜æ¥è‡ªä¸åŒæ–¹å‘çš„å¤¹å‡»
        grad_collided = gradients[mask_collision]
        # è¿™é‡Œç”¨ç®€åŒ–çš„ä½™å¼¦ç›¸ä¼¼åº¦æ¨¡æ‹Ÿèšç±»æ£€æµ‹
        # å®é™…ä»£ç å¯èƒ½éœ€è¦ K-Meansï¼Œè¿™é‡Œä¸ºäº†å¯å¯¼æ€§ç”¨æ–¹å‘æ–¹å·®ä»£æ›¿
        grad_variance = torch.var(grad_collided, dim=0).sum()
        
        is_squeezed = grad_variance > 0.5 # é˜ˆå€¼åˆ¤å®š N_cluster > 1
        
        if is_squeezed:
            # 2. è®¡ç®—ç›®æ ‡ç¼©æ”¾ (Target Scale)
            # Eq 7: g(|d|, u) - s
            # è¿™é‡Œçš„ç›´è§‰æ˜¯ï¼šå¦‚æœç¢°æ’æ·±ï¼Œå°±è¦å¤§å¹…å‡å° scale
            current_scale = obj.scale
            # ç®€åŒ–çš„æƒ©ç½šï¼šç¢°æ’è¶Šæ·±ï¼ŒLoss è¶Šå¤§ï¼Œæ¢¯åº¦ä¼šæŒ‡å‘ç¼©å° scale
            d_abs = torch.abs(sdf[mask_collision])
            return torch.sum(d_abs ** 2) 
        return 0.0

    def _compute_stability_loss(self, obj, scene_graph):
        """
        å¯¹åº” L_stability
        å…¬å¼: Sum(1 - exp(-d^2))
        """
        # 1. é‡‡æ ·åº•éƒ¨ç‚¹ V^B
        bottom_points = self._sample_bottom_points(obj)
        
        # 2. æŸ¥è¯¢çˆ¶èŠ‚ç‚¹è¡¨é¢çš„ SDF (Parent Surface SDF)
        # ä¾‹å¦‚ï¼šæ¯å­åº•éƒ¨ç‚¹ç›¸å¯¹äºæ¡Œé¢çš„ SDF
        parent_sdf, _ = self._query_parent_sdf(bottom_points, obj, scene_graph)
        
        # 3. è®¡ç®— Loss
        # å½“ d=0 (ç´§è´´) æ—¶ï¼Œexp(0)=1ï¼ŒLoss=0
        # å½“ d å¤§ (æ‚¬ç©º) æ—¶ï¼Œexp(-d^2)->0ï¼ŒLoss->1
        return torch.sum(1.0 - torch.exp(-parent_sdf ** 2))

    def _compute_pose_loss(self, scene_graph, ref_img):
        """
        å¯¹åº” L_pose = Î»2d*L2d + Î»3d*L3d
        ä½¿ç”¨ RoMa è¿›è¡ŒåŒ¹é…
        """
        # æ¸²æŸ“å½“å‰ç‰©ä½“å¾—åˆ° rendered_img
        # è¿è¡Œ RoMa(rendered_img, ref_img) å¾—åˆ° matches
        # ç­›é€‰ confidence > 0.6 çš„ç‚¹
        # è®¡ç®— MSE Loss
        pass # éœ€è¦å¯å¾®æ¸²æŸ“å™¨æ”¯æŒ
        
    # --- Helper Placeholders ---
    def _sample_surface(self, obj, n): return torch.randn(n, 3)
    def _sample_bottom_points(self, obj): return torch.randn(50, 3)
    def _query_scene_sdf(self, points, graph): return torch.randn(len(points)), torch.randn(len(points), 3)
    def _query_parent_sdf(self, points, obj, graph): return torch.randn(len(points)), None


3.1 ç¼ºå¤±æ¨¡å—ï¼šScene Judge (åœºæ™¯è£åˆ¤)
ä¾æ®ï¼š è®ºæ–‡ Section 3.4 æ˜ç¡®æåˆ°ï¼š"After iteratively optimizing... a scene judge powered by GPT-4o evaluates the spatial alignment... comparing the generated scene [with] image guidance."
ä½œç”¨ï¼š ä¼˜åŒ–ç»“æŸåï¼Œä¸èƒ½ç›²ç›®è¾“å‡ºã€‚éœ€è¦ GPT-4o æ¥çœ‹ä¸€çœ¼æœ€ç»ˆç»“æœï¼Œåˆ¤æ–­æ˜¯å¦â€œå¯¹å‘³â€ã€‚å¦‚æœè¯„åˆ†å¤ªä½ï¼Œå¯èƒ½éœ€è¦é‡æ–°åšã€‚
ä»£ç å®ç° (SceneJudge ç±»)ï¼š
Python
class SceneJudge:
    def __init__(self):# è£åˆ¤æ˜¯ GPT-4o
        print("Initializing GPT-4o Scene Judge...")

    def evaluate(self, generated_3d_snapshot, guidance_image):"""
        å¯¹åº” Section 3.4: "design three metrics"
        1. Object Category Accuracy
        2. Spatial Relationship Consistency
        3. Visual Similarity
        """
        prompt = """
        You are a Scene Judge. Compare the 'Generated 3D Scene View' with the 'Guidance Image'.
        Evaluate on 3 criteria:
        1. Do the objects match? (Accuracy)
        2. Is the layout/positioning consistent? (Spatial Coherence)
        3. Is there physical inter-penetration? (Physical Plausibility)
        
        Output a Score (0-10) and a Decision (Pass/Refine).
        """# è°ƒç”¨ GPT-4o Vision API å‘é€ä¸¤å¼ å›¾# return response_json
        print("  [Judge] GPT-4o is reviewing the final layout...")
        
        return {"score": 8.5, "decision": "PASS"}

---
3.2 åŸºç¡€è®¾æ–½ï¼šSDF æ•°æ®é¢„å¤„ç† (The Data Prep)
ä¾æ®ï¼š ä½ çš„ PhysicsOptimizer é‡Œä½¿ç”¨äº† sdf[mask_collision]ã€‚ä½†åœ¨ä»£ç é‡Œè¿™åªæ˜¯ä¸ªå ä½ç¬¦ã€‚
ç°å®é—®é¢˜ï¼š ç½‘ä¸Šä¸‹è½½çš„ .obj æˆ– .glb æ¨¡å‹æ˜¯ç½‘æ ¼ (Mesh)ï¼Œä¸æ˜¯ SDFã€‚ä½ æ— æ³•ç›´æ¥æŸ¥è¯¢ Mesh çš„è´Ÿå€¼ã€‚
ä½ éœ€è¦è¡¥å……çš„å†…å®¹ï¼š åœ¨åŠ è½½èµ„äº§åº“æ—¶ï¼Œå¿…é¡»æœ‰ä¸€ä¸ªé¢„è®¡ç®— (Baking) æ­¥éª¤ã€‚
ä»£ç é€»è¾‘è¡¥å……ï¼š
Python
import pytorch3d.structures
from pytorch3d.ops import sample_points_from_meshes

def preprocess_asset_to_sdf(mesh_path):"""
    "replace 3DBB... with Signed Distance Fields (SDFs)"
    åœ¨åŠ è½½æ¨¡å‹æ—¶è¿è¡Œä¸€æ¬¡
    """# 1. åŠ è½½ Mesh
    mesh = load_objs_as_meshes([mesh_path])
    
    # 2. è½¬æ¢ä¸ºç‚¹äº‘å¹¶è®¡ç®— SDF (ç®€åŒ–ç‰ˆï¼šä½¿ç”¨ç‚¹äº‘è¿‘ä¼¼)# çœŸæ­£çš„ SDF éœ€è¦è®¡ç®—ç©ºé—´ä¸­æ¯ä¸ªç‚¹åˆ° Mesh è¡¨é¢çš„è·ç¦»# å·¥ä¸šç•Œé€šå¸¸ä½¿ç”¨ 'mesh_to_sdf' åº“æˆ– DeepSDF é¢„è®­ç»ƒç½‘ç»œ
    print(f"Baking SDF for {mesh_path}...")
    
    # è¿”å›ä¸€ä¸ªå¯æŸ¥è¯¢çš„ SDF å‡½æ•°æˆ– 3D Gridreturn sdf_grid
- å»ºè®®ï¼š å¤ç°æ—¶ï¼Œå» GitHub æ‰¾ä¸€ä¸ªè½»é‡çº§çš„ mesh-to-sdf åº“ï¼ŒæŠŠæ¯ä¸ªå®¶å…·æ¨¡å‹é¢„å¤„ç†æˆä¸€ä¸ª $64 \times 64 \times 64$ çš„ SDF çŸ©é˜µå­˜èµ·æ¥ã€‚

---
3.3 åŸºç¡€è®¾æ–½ï¼šå¯å¾®æ¸²æŸ“å™¨ (Differentiable Renderer)
ä¾æ®ï¼š è®ºæ–‡ Section 3.3.1 å’Œ Section 6.2.3 æåˆ° $$L_{pose}$$ éœ€è¦ "backpropagating gradients"ã€‚
ç°å®é—®é¢˜ï¼š æ™®é€šçš„æ¸²æŸ“ï¼ˆå¦‚ Blenderï¼‰ä¸å¯å¯¼ï¼Œæ¢¯åº¦ä¼ ä¸å›æ¥ã€‚å¿…é¡»ä½¿ç”¨ PyTorch3D çš„ MeshRendererã€‚
ä½ éœ€è¦è¡¥å……çš„å†…å®¹ï¼š ä¸€ä¸ªæ¸²æŸ“å™¨åŒ…è£…ç±»ï¼Œç”¨äºåœ¨ä¼˜åŒ–å¾ªç¯ä¸­å®æ—¶ç”Ÿæˆå›¾åƒã€‚
ä»£ç é€»è¾‘è¡¥å……ï¼š

# "optimization implementation is based on pytorch3D"
from pytorch3d.renderer import (
    MeshRenderer, MeshRasterizer, SoftPhongShader, 
    RasterizationSettings, PerspectiveCameras
)

class DifferentiableRenderer:def __init__(self, device):# åˆå§‹åŒ–ç›¸æœºå’Œå…‰ç…§
        self.cameras = PerspectiveCameras(device=device)
        self.raster_settings = RasterizationSettings(image_size=512)
        self.renderer = MeshRenderer(
            rasterizer=MeshRasterizer(cameras=self.cameras, raster_settings=self.raster_settings),
            shader=SoftPhongShader(device=device)
        )

    def render_scene(self, scene_graph):"""
        è¾“å…¥: åŒ…å«å½“å‰ T, R, s å‚æ•°çš„åœºæ™¯å›¾
        è¾“å‡º: å¸¦æœ‰æ¢¯åº¦çš„ 2D å›¾åƒå¼ é‡ (Tensor)
        """# å°† scene_graph ä¸­çš„ mesh ç»„åˆæˆä¸€ä¸ª Batch# åº”ç”¨å½“å‰çš„ transforms# images = self.renderer(meshes_world)return images



4.  Pose Alignment Evaluation 

import json

# ==============================================================================
# Section 3.4: ç©ºé—´ä¸€è‡´æ€§è£åˆ¤çš„æ ¸å¿ƒ Prompt
# ==============================================================================
SPATIAL_ALIGNMENT_PROMPT = """
This task involves evaluating the pose alignment between two images in a pair. 
One image serves as the image guidance (GT), while the other is a generated image (Render). Your objective is to measure the pose alignment of the generated image relative to the GT image.

Follow these steps for evaluation:

1. Review Objects in the GT Image: Examine locations, sizes, and orientations. Understand spatial relationships (on top of, inside, under).

2. Evaluate pose alignment based on 3 aspects:
   â€¢ Location and Size Similarity (0-1): Compare placement. (e.g. 1.0 = perfect center match, 0.1 = misplaced on ground).
   â€¢ Orientation Similarity (0-1): Check for tilts or rotations. (e.g. 1.0 = aligned perspective).
   â€¢ Overall Layout Similarity (0-1): Assess visual coherence and hierarchical structure.

3. Exclusions: Do not consider style, appearance, object shape, or texture. Focus solely on POSE ALIGNMENT.

4. Output Format: 
   Save the evaluated scores as a JSON file strictly following this structure:
   {
     "location_size_score": 0.0,
     "orientation_score": 0.0,
     "overall_layout_score": 0.0,
     "reasoning": "Brief explanation..."
   }
"""

class SceneJudge:
    def __init__(self, threshold=0.7):
        self.threshold = threshold
        self.system_prompt = SPATIAL_ALIGNMENT_PROMPT

    def evaluate(self, render_image_b64, guidance_image_b64):
        """
        æ‰§è¡Œè£åˆ¤é€»è¾‘: Render vs Guidance
        """
        print("--- [Scene Judge] Assessing Spatial Alignment (0-1 Scale) ---")
        
        # 1. è°ƒç”¨ GPT-4o Vision (ä¼ªä»£ç )
        # response = call_llm(self.system_prompt, [render_image_b64, guidance_image_b64])
        
        # --- æ¨¡æ‹Ÿ LLM è¿”å›çš„ç»“æœ ---
        result = {
            "location_size_score": 0.85,
            "orientation_score": 0.9,
            "overall_layout_score": 0.8,
            "reasoning": "The sofa is correctly centered, but the chair is slightly rotated compared to the GT."
        }
        
        # 2. å†³ç­–é€»è¾‘ (Pass or Replan)
        # è®ºæ–‡ 3.4: "If any metric falls below a predefined threshold, trigger re-planning"
        scores = [
            result["location_size_score"], 
            result["orientation_score"], 
            result["overall_layout_score"]
        ]
        
        min_score = min(scores)
        avg_score = sum(scores) / len(scores)
        
        print(f"    Scores: Loc={scores[0]}, Ori={scores[1]}, Layout={scores[2]}")
        
        if min_score < self.threshold:
            print(f"!!! FAIL: Metric below threshold {self.threshold}. Triggering RE-PLANNING.")
            return False, result # è§¦å‘é‡åš
        else:
            print(f"*** PASS: Scene Approved. (Avg: {avg_score:.2f}) ***")
            return True, result

This task involves evaluating the pose alignment between two images in a pair. One image serves as the image guidance (GT), while the other is a generated image. Your objective is to measure the pose alignment of the generated image relative to the GT image. Follow these steps for evaluation:
1. Review Objects in the GT Image: Examine all objects in the GT image, focusing on their locations, sizes, and orientations. Understand the spatial relationships among objects, such as on top of, inside, under, etc.
2. Evaluate pose alignment: Assess the similarity between the generated image and the GT image based on the following three aspects:
    â€¢ Location and Size Similarity: Compare the location and size of objects in the generated image with those in the GT image. Assign a similarity score between 0 and 1, where 1 indicates the highest similarity. For example:â€“ If an apple in the GT image is placed at the center of a table, and in the generated image it is placed on the left side of the table, the similarity might be moderate (e.g., 0.5).â€“ If the apple is misplaced (e.g., on the ground or missing entirely), the similarity would be very low (e.g., 0.1).
    â€¢ Orientation Similarity: Examine the orientation of each object in the generated image compared to the GT image. Pay close attention to details, noting any deviations such as slight tilts (e.g., right/left, up/down) or rotations that create different perspectives. Assign a score from 0 to 1, where 1 indicates perfect alignment and 0 indicates a significant mismatch (e.g., opposite orientation).
    â€¢ Overall Layout Similarity: Assess the overall visual coherence of the generated image compared to the GT image, including spatial relationships and hierarchical structure. Assign a similarity score between 0 and 1, where 1 represents a perfect match. For instance:â€“ A perfect match occurs when the generated image maintains the same spatial relationships, relative locations, sizes, and orientations as the GT image (e.g., an apple placed at the center of a table in both images).â€“ Small deviations in placement or orientation are acceptable but should result in a lower score.
3. Exclusions: Do not consider style, appearance, object shape, or texture in your evaluation. Focus solely on pose
alignment.
4. Output Format: Clearly document your similarity scores for each aspect (Location and Size Similarity, Orientation Similarity, and Overall Layout Similarity) following the format: location and size similarity score is {}, orientation similarity score is {}, and overall layout similarity score is {}. Please save the evaluated scores as a json file.